{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing of Drug Dataset From Dreammarket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import langdetect as ld\n",
    "import stanza\n",
    "from stanza.pipeline.processor import register_processor, Processor\n",
    "\n",
    "import time\n",
    "from faker import Faker\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import phonenumbers\n",
    "import random\n",
    "import csv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset and remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:  Index(['dream_id', 'vendor', 'name', 'description'], dtype='object')  shape:  (45446, 4)\n"
     ]
    }
   ],
   "source": [
    "path_dream_json = \"Drug_export_50000_final.json\"\n",
    "dnm = pd.read_json(path_dream_json)\n",
    "\n",
    "dnm.rename(columns = {'idproduct':'dream_id', 'seller_name':'vendor', 'product_name':'name', 'category':'subcategory' }, inplace=True)\n",
    "\n",
    "DrugColsToKeep = [ 'dream_id', 'vendor', 'name', 'description',]\n",
    "dnm = dnm[DrugColsToKeep]\n",
    "print(\"Columns: \", dnm.columns, \" shape: \", dnm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO SHALL WE Load the grams data as well?? Currently not loaded since it contains other things than drugs as well\n",
    "\n",
    "Load Abraxas, Agora, Alpha, Me and Oxygen  \n",
    "DO NOT LOAD NK and Silkkitie since their data csv is quite noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npath_grams_dataset = [\"grams/Abraxas.csv\", \"grams/Agora.csv\", \"grams/Alpha.csv\", \"grams/ME.csv\", \"grams/Oxygen.csv\"]\\n\\n\\ngram_list = []\\nfor filename in path_grams_dataset:\\n    df = pd.read_csv(filename)\\n    gram_list.append(df)\\n\\ngrams = pd.concat(gram_list, axis=0, ignore_index=True)\\ngrams.rename(columns = {\\'vendor_name\\':\\'vendor\\'}, inplace=True)\\ngrams = grams[DrugColsToKeep]\\nprint(\"Columns: \", grams.columns, \" shape: \", grams.shape)\\n#dnm = pd.concat([dnm, grams], axis=0, ignore_index=True) # TODO Shall I add Grams Data as well? This would add noise!\\n\\nprint(\"Columns: \", grams.columns, \" shape: \", grams.shape)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "path_grams_dataset = [\"grams/Abraxas.csv\", \"grams/Agora.csv\", \"grams/Alpha.csv\", \"grams/ME.csv\", \"grams/Oxygen.csv\"]\n",
    "\n",
    "\n",
    "gram_list = []\n",
    "for filename in path_grams_dataset:\n",
    "    df = pd.read_csv(filename)\n",
    "    gram_list.append(df)\n",
    "\n",
    "grams = pd.concat(gram_list, axis=0, ignore_index=True)\n",
    "grams.rename(columns = {'vendor_name':'vendor'}, inplace=True)\n",
    "grams = grams[DrugColsToKeep]\n",
    "print(\"Columns: \", grams.columns, \" shape: \", grams.shape)\n",
    "#dnm = pd.concat([dnm, grams], axis=0, ignore_index=True) # TODO Shall I add Grams Data as well? This would add noise!\n",
    "\n",
    "print(\"Columns: \", grams.columns, \" shape: \", grams.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines before duplicate removal: 45446\n",
      "lines After duplicate removal: 20434\n"
     ]
    }
   ],
   "source": [
    "#Duplicate removal:\n",
    "print(\"lines before duplicate removal:\", dnm.shape[0])\n",
    "dnm.drop_duplicates('description', inplace=True)\n",
    "print(\"lines After duplicate removal:\", dnm.shape[0])\n",
    "\n",
    "#Create unique index again\n",
    "dnm.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove other languages, empty and non-string descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 21s, sys: 16.4 s, total: 4min 38s\n",
      "Wall time: 4min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def checkLang(text):\n",
    "    try:\n",
    "        if isinstance(text, str):\n",
    "            return ld.detect(text)\n",
    "        else:\n",
    "            return 'No String'\n",
    "    except:  \n",
    "        return 'Error in detect'\n",
    "\n",
    "dnm['lang'] = dnm.apply(lambda row: checkLang(row['description']), axis=1) # TODO shall we work with other languages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Before Language Removal\n",
      "(20434, 5)\n",
      "Shape AFTER Language Removal\n",
      "(18332, 5)\n"
     ]
    }
   ],
   "source": [
    "print('Shape Before Language Removal')\n",
    "print(str(dnm.shape))\n",
    "dnm_en = dnm[dnm['lang'] == 'en'].copy()\n",
    "print('Shape AFTER Language Removal')\n",
    "print(str(dnm_en.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove very short/long texts or texts with similiar start\n",
    "\n",
    "Calculating the edit distance is super cumbersume --> approx. a day runtime with only edit distance (and edit distance doesn't) Take into account that a char can be inserted.. (\"apple\" and \"1apple\" has an edit distance of 5)\n",
    "levenstein or hamming etc. (which compensate for inserted chars) would take even longer.. \n",
    "Therefore, this small heuristc was found to remove texts where the beginning is equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Before Short Descr. Removal\n",
      "(18332, 5)\n",
      "Shape After Short Descr. Removal\n",
      "(16844, 5)\n"
     ]
    }
   ],
   "source": [
    "#Very small descriptions are not interesting for us since we try to find named entities in sentences..\n",
    "print('Shape Before Short Descr. Removal')\n",
    "print(str(dnm_en.shape))\n",
    "dnm_en = dnm_en[dnm_en['description'].map(len) > 30].copy()\n",
    "dnm_en = dnm_en[dnm_en['description'].map(len) < 3000].copy()#Suuper long texts usually contain noisy crap\n",
    "print('Shape After Short Descr. Removal')\n",
    "print(str(dnm_en.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We take the first 200 chars and remove everything non-alphabetical from them. if they are equal to some other line we remove it. Therefore we can be quite sure that we don't have very similiar drug ads where only the amount of the drug or something like this changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Before Start/End Duplicate Removal\n",
      "(16844, 6)\n",
      "Shape After Start/End Duplicate Removal\n",
      "(11672, 6)\n"
     ]
    }
   ],
   "source": [
    "dnm_en['descr_start'] = dnm_en['description'].str[:100].apply(lambda text: re.sub(r'[^A-Za-z]+', '', text))\n",
    "print('Shape Before Start/End Duplicate Removal')\n",
    "print(str(dnm_en.shape))\n",
    "dnm_en.drop_duplicates('descr_start', inplace=True)\n",
    "print('Shape After Start/End Duplicate Removal')\n",
    "print(str(dnm_en.shape))\n",
    "\n",
    "#Remove helper Columns\n",
    "del dnm_en['descr_start'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudonymize \n",
    "Remove telephone numbers and vendor names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a random phone number\n",
    "def rand_phone():\n",
    "    return \"+%d %d %d %d\" % (random.randint(20,99), random.randint(100,999), random.randint(1000,9999), random.randint(100,999))\n",
    "\n",
    "\n",
    "# create random name\n",
    "def get_fake_name():\n",
    "    # can create fake names\n",
    "    fake = Faker()\n",
    "    fake_name = fake.name().replace(\" \", \"\")\n",
    "    if random.randint(1, 10) % 2 == 0:\n",
    "        fake_name = fake_name.lower()\n",
    "    if random.randint(1, 10) % 3 == 0:\n",
    "        fake_name = \"%s%d\" % (fake_name.lower(), random.randint(10, 99))\n",
    "    return fake_name\n",
    "\n",
    "\n",
    "# \n",
    "def pseudonymize(df, name_column): \n",
    "    # create a list of all vendor names with different Capitalisation\n",
    "    names_list = df['vendor'].unique()\n",
    "    names_list = [re.escape(str(x)) for x in names_list] # OLD [str(x) for x in names_list] \n",
    "    capitalize = [x.capitalize() for x in names_list]\n",
    "    lower = [x.lower() for x in names_list]\n",
    "    upper = [x.upper() for x in names_list]\n",
    "    names_list += capitalize + lower + upper\n",
    "    \n",
    "    \n",
    "    if 'vendor' in df.columns:\n",
    "        # remove empty strings\n",
    "        df.replace('', np.nan, inplace=True)\n",
    "        \n",
    "        # replace all vendor names by dummie\n",
    "        name_replacements = {name: get_fake_name() for name in names_list  if name is not np.nan}\n",
    "        df.replace({name_column: name_replacements}, inplace=True)\n",
    "        df.replace({\"description\": name_replacements}, inplace=True, regex=True)\n",
    "       \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all phonenumbers \n",
    "all_phonenumbers = [match.raw_string for match in phonenumbers.PhoneNumberMatcher(\" \".join(dnm_en.description.unique()), \"INTERNATIONAL\")]\n",
    "all_phonenumbers = (list(set(all_phonenumbers)))\n",
    "# Replace all phonenumbers\n",
    "for number in all_phonenumbers:\n",
    "    dnm_en.description = dnm_en.description.apply(lambda x:x.replace(number,rand_phone()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_mails(text):\n",
    "    domain = ['.com','.de','.ru','.org']\n",
    "    vendor = ['gmail', 'air','wing','microsoft','hotmail','outlook']\n",
    "\n",
    "    mail = get_fake_name() + '@' + vendor[random.randint(0,len(vendor)-1)] + domain[random.randint(0,len(domain)-1)] \n",
    "    return re.sub(r'\\S+@\\S+\\s?', mail, text)\n",
    "\n",
    "dnm_en.description = dnm_en.description.apply(lambda text: replace_mails(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 12s, sys: 35.3 s, total: 6min 47s\n",
      "Wall time: 6min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# vendor name  bcdirect is replaced by random name \n",
    "\n",
    "dnm_pseudo = pseudonymize(dnm_en.copy(),\"vendor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanza Processing \n",
    "\n",
    "### Create special char remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted_elements(text):\n",
    "    final_text=text\n",
    "    final_text = re.sub(r'https?:\\/\\/\\S*[\\r\\n]*', '', final_text)\n",
    "    final_text = re.sub(r'\\S*.onion\\S*[\\r\\n]*', '', final_text)\n",
    "    final_text = re.sub(r'[\\+!~@#$%^&*()={}\\[\\]:;<.>?\\'\"]', '', final_text)\n",
    "    final_text = re.sub(r'[-]+', '-', final_text)\n",
    "    final_text = re.sub(r'[_]+', '_', final_text)\n",
    "    return final_text\n",
    "\n",
    "\n",
    "\n",
    "@register_processor(\"customcleaner\")\n",
    "class cleanerProcessor(Processor):\n",
    "    ''' Processor removes all special chars we do not appreciate and links '''\n",
    "    _requires = set(['tokenize']) # Shouldn#t we do that before\n",
    "    _provides = set(['specialCharRemover'])\n",
    "\n",
    "    def __init__(self, config, pipeline, use_gpu):\n",
    "        pass\n",
    "\n",
    "    def _set_up_model(self, *args):\n",
    "        pass\n",
    "\n",
    "    def process(self, doc):\n",
    "        \n",
    "        doc.text = doc.text # Question to Gabor SHALL the original text stay the same?\n",
    "        for sent in doc.sentences:\n",
    "            for tok in sent.tokens:\n",
    "                tok.text = remove_unwanted_elements(tok.text)\n",
    "                \n",
    "            for word in sent.words:\n",
    "                word.text = remove_unwanted_elements(word.text)\n",
    "\n",
    "                \n",
    "            #remove empty tokens/words    \n",
    "            sent.tokens = [tok for tok in sent.tokens if len(tok.text) > 0]\n",
    "            sent.words = [word for word in sent.words if len(word.text) > 0]\n",
    "        #remove empty sentences\n",
    "        doc.sentences = [sent for sent in doc.sentences if len(sent.tokens) > 0]\n",
    "\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanza Segmentation\n",
    "\n",
    "TODO Talk with Gabor if we should really separate sentences according to Stanza.. Imho this will result in a lot of small sentences with very little context. Does this matter? In case of  Annotating something like Drug_mention it would.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-03 10:59:19 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor     | Package |\n",
      "---------------------------\n",
      "| tokenize      | ewt     |\n",
      "| customcleaner | default |\n",
      "===========================\n",
      "\n",
      "2021-02-03 10:59:19 INFO: Use device: gpu\n",
      "2021-02-03 10:59:19 INFO: Loading: tokenize\n",
      "2021-02-03 10:59:25 INFO: Loading: customcleaner\n",
      "2021-02-03 10:59:25 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = stanza.Pipeline('en', processors='tokenize,customcleaner')#,specialchars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 15s, sys: 2.62 s, total: 8min 18s\n",
      "Wall time: 8min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dnm_pseudo['doc'] =  dnm_pseudo.description.apply(lambda text:  tokenizer(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore Stanza to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_tokenized_text(doc):\n",
    "    cleaned_text= \"\"\n",
    "    for sentence in doc.sentences:\n",
    "        for token_dict in sentence.words:\n",
    "            cleaned_text += token_dict.text + \" \"\n",
    "        cleaned_text = cleaned_text.strip() + \". \"\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "dnm_pseudo[\"clean_description\"] = dnm_pseudo[\"doc\"].apply(lambda doc:  restore_tokenized_text(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11672, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle the dataset to prevent any order with meaning\n",
    "df_shuffled=dnm_pseudo.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "df_shuffled.to_pickle('intermediate_store/preprocessed_ads_V1.1.pickle')\n",
    "dnm_pseudo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled = pd.read_pickle('intermediate_store/preprocessed_ads_V1.1.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate item listings according to length into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_textLength(text):\n",
    "    if len(text)<100:\n",
    "        return 1\n",
    "    elif len(text)<250:\n",
    "        return 2\n",
    "    elif len(text)<500:\n",
    "        return 3\n",
    "    elif len(text)<750:\n",
    "        return 4\n",
    "    elif len(text)<1000:\n",
    "        return 5\n",
    "    elif len(text)<1500:\n",
    "        return 6\n",
    "    elif len(text)<2000:\n",
    "        return 7\n",
    "    elif len(text)<2500:\n",
    "        return 8\n",
    "    elif len(text)<3000:\n",
    "        return 9\n",
    "    \n",
    "df_shuffled[\"textLengthCat\"] = df_shuffled[\"clean_description\"].apply(lambda text: categorize_textLength(text))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    2784\n",
       "3.0    2475\n",
       "1.0    1972\n",
       "4.0    1347\n",
       "6.0    1041\n",
       "5.0     948\n",
       "7.0     585\n",
       "8.0     347\n",
       "9.0     172\n",
       "Name: textLengthCat, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shuffled[\"textLengthCat\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled[df_shuffled['textLengthCat'] <= 5][['dream_id', 'name', 'clean_description']].to_csv(\"final_data/shortTextDrugsV1.0.tsv\", sep = '\\t')\n",
    "df_shuffled[df_shuffled['textLengthCat'] > 5][['dream_id', 'name', 'clean_description']].to_csv(\"final_data/longTextDrugsV1.0.tsv\", sep = '\\t') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NER36",
   "language": "python",
   "name": "ner36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
