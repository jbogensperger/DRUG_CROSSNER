{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "from stanza.utils.conll import CoNLL\n",
    "from stanza.models.common.doc import Document\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINAL RESULT DOC key:dream_id, value:stanza_doc\n",
    "final_data={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPEN PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data used for dataset creation\n",
    "df_shuffled_old = pd.read_pickle('intermediate_store/preprocessed_ads_V1.0.pickle')\n",
    "\n",
    "#Load reviewed annotation data Appen\n",
    "with open('reviewed_data/initial_1300.json') as json_file:\n",
    "    reviewed_appen = json.load(json_file)\n",
    "\n",
    "df_shuffled_old[\"new_doc\"]=\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_with_indexes_appen(string):\n",
    "    token_list = string.split(' ')\n",
    "    tuple_list = []\n",
    "    curr_start = 0\n",
    "    for token in token_list:\n",
    "        tuple_list.append( (token, curr_start, curr_start+len(token)-1) )#real end char, not start of next one\n",
    "        curr_start += len(token) + 1\n",
    "    return tuple_list\n",
    "def annotate_doc_appen(review_json, doc):\n",
    "    \n",
    "    #create splitted review token list\n",
    "    #create a tag list with same length\n",
    "    reviewed_tuples = split_with_indexes_appen(review_json['data']['clean_description'])\n",
    "    tag_list_rev = ['O' for i in range(len(reviewed_tuples))]\n",
    "    \n",
    "    #For each annotation check which tokens shall be labelled\n",
    "    for tag_dict in review_json['completions'][0]['result']:\n",
    "        first=True\n",
    "        \n",
    "        for j, (token, r_start, r_end) in enumerate(reviewed_tuples):\n",
    "            tag_start = tag_dict['value']['start']\n",
    "            tag_end = tag_dict['value']['end'] - 1 #real end char, not start of next one\n",
    "            #print('r_start {} r_end {} tag_start {} tag_end {} token: {}'.format(r_start, r_end, tag_start, tag_end, token))\n",
    "            if tag_end >= r_start and tag_start <= r_end and tag_dict['value']['labels'] == ['Drug']:\n",
    "                if first:\n",
    "                    tag_list_rev[j] = 'B-Drug'\n",
    "                    first=False\n",
    "                else:\n",
    "                    tag_list_rev[j] = 'I-Drug'\n",
    "    \n",
    "    #loop over doc and annotate the tokens in misc.\n",
    "    token_id=0\n",
    "    for sentence in doc:\n",
    "        for token_dict in sentence:\n",
    "            token_dict['misc'] = token_dict['misc'] + '|label=' + tag_list_rev[token_id]\n",
    "            token_id+=1\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length Problem at pandas_id: 742 and json item nr: 344\n",
      "Detail Problem at pandas_id: 742 and json item nr: 344\n",
      "Length Problem at pandas_id: 802 and json item nr: 1105\n",
      "1307 rows had still the same amount of tokens. 2 not. this is 0.001530221882172915 %\n",
      "1308 rows had exactly the same tokens. 1 not. this is 0.0007645259938837921 %\n",
      "1016 docs are in the final result\n"
     ]
    }
   ],
   "source": [
    "amt_same_len = 0\n",
    "amt_not_same_len = 0\n",
    "amt_exactly_same = 0\n",
    "amt_not_exactly_same = 0\n",
    "\n",
    "for i, review in enumerate(reviewed_appen):\n",
    "    p_id = review['data']['p_id']\n",
    "    flat_list_orig = [item for sublist in df_shuffled_old.loc[p_id].doc.to_dict() for item in sublist]\n",
    "    flat_list_rev =review['data']['clean_description'].split(' ')\n",
    "    length_problem=False\n",
    "    \n",
    "    #Check if length matches\n",
    "    if len(flat_list_orig) == len(flat_list_rev):\n",
    "        amt_same_len+=1\n",
    "    else:\n",
    "        amt_not_same_len +=1\n",
    "        print('Length Problem at pandas_id: {} and json item nr: {}'.format(p_id, i))\n",
    "        length_problem=True\n",
    "    \n",
    "    #Check each token if they are the equal:\n",
    "    same = True\n",
    "    for j, rev_span in enumerate(flat_list_rev):\n",
    "        if not flat_list_orig[j]['text']==rev_span:\n",
    "            if not ( flat_list_orig[j]['text'] == rev_span[:-1] and rev_span[-1:] == '.' ):\n",
    "                same = False\n",
    "                print('Detail Problem at pandas_id: {} and json item nr: {}'.format(p_id, i))\n",
    "                break\n",
    "    if same:\n",
    "        amt_exactly_same+=1\n",
    "        #If length and exact tokens are ok --> add label!\n",
    "        if not length_problem:\n",
    "            new_doc = annotate_doc_appen(review, df_shuffled_old.loc[p_id].doc.to_dict())\n",
    "            new_doc = Document(new_doc)\n",
    "            df_shuffled_old.at[p_id, 'new_doc'] = new_doc\n",
    "            final_data[review['data']['dream_id']]=new_doc\n",
    "    else:\n",
    "        amt_not_exactly_same+=1\n",
    "        \n",
    "print('{} rows had still the same amount of tokens. {} not. this is {} %'.format(amt_same_len, amt_not_same_len, amt_not_same_len/amt_same_len ))\n",
    "print('{} rows had exactly the same tokens. {} not. this is {} %'.format(amt_exactly_same, amt_not_exactly_same, amt_not_exactly_same/amt_exactly_same))\n",
    "print('%d docs are in the final result'%(len(final_data.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1016, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shuffled_old = df_shuffled_old[df_shuffled_old['new_doc'] != \"\"]\n",
    "df_shuffled_old.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon MTurk Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled_new = pd.read_pickle('intermediate_store/preprocessed_AMT_ads_V1.2.pickle')\n",
    "\n",
    "df_shuffled_new[\"new_doc\"]=\"\"\n",
    "\n",
    "filename_amt='reviewed_data/AMT_Reviewed_Data_270321.json'\n",
    "\n",
    "#Load reviewed annotation data\n",
    "with open(filename_amt) as json_file:\n",
    "    reviewed_amt = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled_new['clean_description'] = df_shuffled_new['clean_description'].str.replace('&#44', ',')\n",
    "df_shuffled_new['clean_description'] = df_shuffled_new['clean_description'].str.replace('&#39', '\\'')\n",
    "df_shuffled_new['clean_description'] = df_shuffled_new['clean_description'].str.replace('&#34', '\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse HTML encoding of characters, which was necessary for AMT platform\n",
    "def replace_char_in_doc(char, replacement, doc):\n",
    "    new_doc=doc.to_dict()\n",
    "    for i, sentence in enumerate(new_doc):\n",
    "        for j, token in enumerate(sentence):\n",
    "            #print('i:%d, j:%d'%(i, j))\n",
    "            new_doc[i][j]['text'] = token['text'].replace(char, replacement)\n",
    "        \n",
    "    return Document(new_doc)\n",
    " \n",
    "df_shuffled_new['doc'] = df_shuffled_new['doc'].apply(lambda doc: replace_char_in_doc('&#44', ',', doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_with_indexes_amt(string):\n",
    "    token_list = string.split(' ')\n",
    "    tuple_list = []\n",
    "    curr_start = 0\n",
    "    for token in token_list:\n",
    "        tuple_list.append( (token, curr_start, curr_start+len(token)-1) )#real end char, not start of next one\n",
    "        curr_start += len(token) + 1\n",
    "    return tuple_list\n",
    "def annotate_doc_amt(review_json, doc):\n",
    "    \n",
    "    #create splitted review token list\n",
    "    #create a tag list with same length\n",
    "    reviewed_tuples = split_with_indexes_amt(review_json['data']['clean_description'])\n",
    "    tag_list_rev = ['O' for i in range(len(reviewed_tuples))]\n",
    "    \n",
    "    #For each annotation check which tokens shall be labelled\n",
    "    for tag_dict in review_json['completions'][0]['result']:\n",
    "        first=True\n",
    "        \n",
    "        for j, (token, r_start, r_end) in enumerate(reviewed_tuples):\n",
    "            tag_start = tag_dict['value']['start']\n",
    "            tag_end = tag_dict['value']['end'] - 1 #real end char, not start of next one\n",
    "            #print('r_start {} r_end {} tag_start {} tag_end {} token: {}'.format(r_start, r_end, tag_start, tag_end, token))\n",
    "            if tag_end >= r_start and tag_start <= r_end and tag_dict['value']['labels'] == ['Drug']:\n",
    "                if first:\n",
    "                    tag_list_rev[j] = 'B-Drug'\n",
    "                    first=False\n",
    "                else:\n",
    "                    tag_list_rev[j] = 'I-Drug'\n",
    "    \n",
    "    #loop over doc and annotate the tokens in misc.\n",
    "    token_id=0\n",
    "    for sentence in doc:\n",
    "        for token_dict in sentence:\n",
    "            token_dict['misc'] = token_dict['misc'] + '|label=' + tag_list_rev[token_id]\n",
    "            token_id+=1\n",
    "    return doc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length Problem at pandas_id: 384 and json item nr: 415\n",
      "Detail Problem at pandas_id: 384 and json item nr: 415\n",
      "Orig_flat_list: - - and rev_span: -\n",
      "Length Problem at pandas_id: 883 and json item nr: 916\n",
      "Detail Problem at pandas_id: 883 and json item nr: 916\n",
      "Orig_flat_list: \u0001 and rev_span: Grandaddy\n",
      "Length Problem at pandas_id: 2188 and json item nr: 1007\n",
      "Detail Problem at pandas_id: 2188 and json item nr: 1007\n",
      "Orig_flat_list: q0qLeBxj553tAM2cjZCqxPWIVJwunVDEdpzlUroFXXmgOI7A9myk9CqagqNFw1i 2TXfe5nLlinNdxEKNCeQehhWeryPY5/WC5PRABEBAAG0GkNhbmphbTQyMCA8bWVA and rev_span: q0qLeBxj553tAM2cjZCqxPWIVJwunVDEdpzlUroFXXmgOI7A9myk9CqagqNFw1i\n",
      "Length Problem at pandas_id: 2081 and json item nr: 1295\n",
      "Length Problem at pandas_id: 1722 and json item nr: 1641\n",
      "Detail Problem at pandas_id: 1722 and json item nr: 1641\n",
      "Orig_flat_list: - - and rev_span: -\n",
      "Length Problem at pandas_id: 3637 and json item nr: 1673\n",
      "Detail Problem at pandas_id: 3637 and json item nr: 1673\n",
      "Orig_flat_list: q0qLeBxj553tAM2cjZCqxPWIVJwunVDEdpzlUroFXXmgOI7A9myk9CqagqNFw1i 2TXfe5nLlinNdxEKNCeQehhWeryPY5/WC5PRABEBAAG0GkNhbmphbTQyMCA8bWVA and rev_span: q0qLeBxj553tAM2cjZCqxPWIVJwunVDEdpzlUroFXXmgOI7A9myk9CqagqNFw1i\n",
      "Length Problem at pandas_id: 2628 and json item nr: 1822\n",
      "Detail Problem at pandas_id: 2628 and json item nr: 1822\n",
      "Orig_flat_list: q0qLeBxj553tAM2cjZCqxPWIVJwunVDEdpzlUroFXXmgOI7A9myk9CqagqNFw1i 2TXfe5nLlinNdxEKNCeQehhWeryPY5/WC5PRABEBAAG0GkNhbmphbTQyMCA8bWVA and rev_span: q0qLeBxj553tAM2cjZCqxPWIVJwunVDEdpzlUroFXXmgOI7A9myk9CqagqNFw1i\n",
      "Length Problem at pandas_id: 990 and json item nr: 2188\n",
      "Detail Problem at pandas_id: 990 and json item nr: 2188\n",
      "Orig_flat_list: xHBS8so/hAol1ZRgBpMqD9sQ3wPgK5cabp7uJRR0QcWoeIQjyuDZU5EAiYGozCAF 3byr2MMQijMZlhFMQwxRzwIPP7KetH5t8kZh2v32STmqxvjp4RAuWXHRLgsr and rev_span: xHBS8so/hAol1ZRgBpMqD9sQ3wPgK5cabp7uJRR0QcWoeIQjyuDZU5EAiYGozCAF\n",
      "2491 rows had still the same amount of tokens. 8 not. this is 0.003211561621838619 %\n",
      "2492 rows had exactly the same tokens. 7 not. this is 0.0028089887640449437 %\n",
      "3507 docs are in the final result\n"
     ]
    }
   ],
   "source": [
    "amt_same_len = 0\n",
    "amt_not_same_len = 0\n",
    "amt_exactly_same = 0\n",
    "amt_not_exactly_same = 0\n",
    "\n",
    "for i, review in enumerate(reviewed_amt):\n",
    "    p_id = review['data']['p_id']\n",
    "    flat_list_orig = [item for sublist in df_shuffled_new.loc[p_id].doc.to_dict() for item in sublist]\n",
    "    flat_list_rev =review['data']['clean_description'].split(' ')\n",
    "    length_problem=False\n",
    "    \n",
    "    #Check if length matches\n",
    "    if len(flat_list_orig) == len(flat_list_rev):\n",
    "        amt_same_len+=1\n",
    "    else:\n",
    "        amt_not_same_len +=1\n",
    "        print('Length Problem at pandas_id: {} and json item nr: {}'.format(p_id, i))\n",
    "        length_problem=True\n",
    "    \n",
    "    #Check each token if they are the equal:\n",
    "    same = True\n",
    "    for j, rev_span in enumerate(flat_list_rev):\n",
    "        if not flat_list_orig[j]['text']==rev_span:\n",
    "            if not ( flat_list_orig[j]['text'] == rev_span[:-1] and rev_span[-1:] == '.' ):\n",
    "                same = False\n",
    "                print('Detail Problem at pandas_id: {} and json item nr: {}'.format(p_id, i))\n",
    "                print('Orig_flat_list: {} and rev_span: {}'.format(flat_list_orig[j]['text'], rev_span))\n",
    "                break\n",
    "    if same:\n",
    "        amt_exactly_same+=1\n",
    "        #If length and exact tokens are ok --> add label!\n",
    "        if not length_problem:\n",
    "            new_doc = annotate_doc_amt(review, df_shuffled_new.loc[p_id].doc.to_dict())\n",
    "            new_doc = Document(new_doc)\n",
    "            df_shuffled_new.at[p_id, 'new_doc'] = new_doc\n",
    "            final_data[review['data']['dream_id']]=new_doc\n",
    "    else:\n",
    "        amt_not_exactly_same+=1\n",
    "\n",
    "if amt_not_same_len > 0:\n",
    "    print('{} rows had still the same amount of tokens. {} not. this is {} %'.format(amt_same_len, amt_not_same_len,\n",
    "                                                                                     amt_not_same_len/amt_same_len ))\n",
    "else:\n",
    "    print('{} rows had still the same amount of tokens. {} not. this is {} %'.format(amt_same_len, amt_not_same_len, 1 ))\n",
    "\n",
    "if amt_not_exactly_same > 0:\n",
    "    print('{} rows had exactly the same tokens. {} not. this is {} %'.format(amt_exactly_same, amt_not_exactly_same,\n",
    "                                                                         amt_not_exactly_same/amt_exactly_same))\n",
    "else:\n",
    "    print('{} rows had exactly the same tokens. {} not. this is {} %'.format(amt_exactly_same, amt_not_exactly_same, 1))\n",
    "    \n",
    "print('%d docs are in the final result'%(len(final_data.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2491, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shuffled_new=df_shuffled_new[df_shuffled_new['new_doc'] != \"\"]\n",
    "df_shuffled_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CONLL Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3507, 9)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.concat([df_shuffled_old, df_shuffled_new])\n",
    "final_df.reset_index(inplace=True)\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['conll'] = final_df[\"new_doc\"].apply(lambda doc: CoNLL.convert_dict(doc.to_dict() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store or load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'dream_id', 'name', 'description', 'lang', 'clean_description',\n",
       "       'stanza_doc', 'conll'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del final_df['doc']\n",
    "del final_df['vendor']\n",
    "final_df.rename(columns={\"new_doc\": \"stanza_doc\"}, inplace=True)\n",
    "#final_df.to_pickle('intermediate_store/final_df_v1.0.pickle')\n",
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_pickle('intermediate_store/final_df_v1.0.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2244 lines, dev: 561 lines, test: 702 lines\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(final_df, test_size=0.2, random_state=42)\n",
    "train, dev = train_test_split(train, test_size=0.2, random_state=42)\n",
    "print('Train: %d lines, dev: %d lines, test: %d lines'%(len(train), len(dev), len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'dream_id', 'name', 'description', 'lang', 'clean_description',\n",
       "       'stanza_doc', 'conll'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def write_conll_to_file(df, filename, with_comments, full_conll_format):\n",
    "    \n",
    "    with io.open(filename, 'w', encoding='utf8') as fout:\n",
    "    \n",
    "        for i, row in df.iterrows():\n",
    "            \n",
    "            if with_comments:\n",
    "                fout.write('#NAME\\t' + row['name'] +'\\n')\n",
    "                fout.write('#original_description\\t' + row['description'] + '\\n')\n",
    "                fout.write('#dream_id\\t' + str(row['dream_id']) + '\\n')\n",
    "                \n",
    "            for sentence in row['conll']:\n",
    "                for line in sentence:\n",
    "                    \n",
    "                    if full_conll_format: #write all CONLL columns \n",
    "                        for j, token in enumerate(line):\n",
    "                            \n",
    "                            if j >0:\n",
    "                                 fout.write('\\t')\n",
    "                            fout.write(token)\n",
    "                        fout.write('\\n')\n",
    "                    else:#only write token and label\n",
    "                        label = line[9].split('|')[2].split('=')[1] # line[9] looks like: 'start_char=0|end_char=4|label=O'\n",
    "                        fout.write('{token}\\t{tag}\\n'.format(token=line[1], tag=label))\n",
    "                #Write '.' so I can separate Sentences for long texts.\n",
    "                fout.write('.\\tO\\n')\n",
    "                \n",
    "            #After each item we need an empty line as separator         \n",
    "            fout.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'reviewed_data/training_files/'\n",
    "train_file_path= output_dir + 'train.txt'\n",
    "dev_file_path= output_dir + 'dev.txt'\n",
    "test_file_path= output_dir + 'test.txt'\n",
    "\n",
    "\n",
    "comments = False\n",
    "full_conll=False\n",
    "\n",
    "write_conll_to_file(train, train_file_path, comments, full_conll)\n",
    "write_conll_to_file(dev, dev_file_path, comments, full_conll)\n",
    "write_conll_to_file(test, test_file_path, comments, full_conll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df['conll'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Dataframes as json\n",
    "json_train_file_path= output_dir + 'train.json'\n",
    "json_dev_file_path= output_dir + 'dev.json'\n",
    "json_test_file_path= output_dir + 'test.json'\n",
    "\n",
    "\n",
    "train[['index', 'dream_id', 'name', 'description', 'lang', 'clean_description', 'conll']].to_json(json_train_file_path, orient=\"records\")\n",
    "dev[['index', 'dream_id', 'name', 'description', 'lang', 'clean_description', 'conll']].to_json(json_dev_file_path, orient=\"records\")\n",
    "test[['index', 'dream_id', 'name', 'description', 'lang', 'clean_description', 'conll']].to_json(json_test_file_path, orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NER36",
   "language": "python",
   "name": "ner36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
