{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing of Drug Dataset From Dreammarket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import langdetect as ld\n",
    "import stanza\n",
    "from stanza.pipeline.processor import register_processor, Processor\n",
    "\n",
    "import time\n",
    "from faker import Faker\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import phonenumbers\n",
    "import random\n",
    "import csv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load GRAMS data for the Domain Adaptive Preprocessing Corpus\n",
    "\n",
    "Load Abraxas, Agora, Alpha, Me and Oxygen  \n",
    "DO NOT LOAD NK and Silkkitie since their data csv is quite noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:  Index(['hash', 'vendor', 'name', 'description'], dtype='object')  shape:  (61420, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path_grams_dataset = [\"grams/Abraxas.csv\", \"grams/Agora.csv\", \"grams/Alpha.csv\", \"grams/ME.csv\", \"grams/Oxygen.csv\"]\n",
    "\n",
    "\n",
    "gram_list = []\n",
    "for filename in path_grams_dataset:\n",
    "    df = pd.read_csv(filename)\n",
    "    gram_list.append(df)\n",
    "\n",
    "    \n",
    "DrugColsToKeep = [ 'hash', 'vendor', 'name', 'description',]\n",
    "grams = pd.concat(gram_list, axis=0, ignore_index=True)\n",
    "grams.rename(columns = {'vendor_name':'vendor'}, inplace=True)\n",
    "grams = grams[DrugColsToKeep]\n",
    "print(\"Columns: \", grams.columns, \" shape: \", grams.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines before duplicate removal: 61420\n",
      "lines After duplicate removal: 37078\n"
     ]
    }
   ],
   "source": [
    "#Duplicate removal:\n",
    "print(\"lines before duplicate removal:\", grams.shape[0])\n",
    "grams.drop_duplicates('description', inplace=True)\n",
    "print(\"lines After duplicate removal:\", grams.shape[0])\n",
    "\n",
    "#Create unique index again\n",
    "grams.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove other languages, empty and non-string descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 29s, sys: 30.2 s, total: 6min 59s\n",
      "Wall time: 6min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def checkLang(text):\n",
    "    try:\n",
    "        if isinstance(text, str):\n",
    "            return ld.detect(text)\n",
    "        else:\n",
    "            return 'No String'\n",
    "    except:  \n",
    "        return 'Error in detect'\n",
    "\n",
    "grams['lang'] = grams.apply(lambda row: checkLang(row['description']), axis=1) # TODO shall we work with other languages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Before Language Removal\n",
      "(37078, 5)\n",
      "Shape AFTER Language Removal\n",
      "(34353, 5)\n"
     ]
    }
   ],
   "source": [
    "print('Shape Before Language Removal')\n",
    "print(str(grams.shape))\n",
    "grams = grams[grams['lang'] == 'en'].copy()\n",
    "print('Shape AFTER Language Removal')\n",
    "print(str(grams.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove very short/long texts or texts with similiar start\n",
    "\n",
    "Calculating the edit distance is super cumbersume --> approx. a day runtime with only edit distance (and edit distance doesn't) Take into account that a char can be inserted.. (\"apple\" and \"1apple\" has an edit distance of 5)\n",
    "levenstein or hamming etc. (which compensate for inserted chars) would take even longer.. \n",
    "Therefore, this small heuristc was found to remove texts where the beginning is equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Before Short Descr. Removal\n",
      "(34353, 5)\n",
      "Shape After Short Descr. Removal\n",
      "(32893, 5)\n"
     ]
    }
   ],
   "source": [
    "#Very small descriptions are not interesting for us since we try to find named entities in sentences..\n",
    "print('Shape Before Short Descr. Removal')\n",
    "print(str(grams.shape))\n",
    "grams = grams[grams['description'].map(len) > 30].copy()\n",
    "grams = grams[grams['description'].map(len) < 3000].copy() # In case we take very long descriptions we gotta cut them down to blocks.\n",
    "print('Shape After Short Descr. Removal')\n",
    "print(str(grams.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We take the first 200 chars and remove everything non-alphabetical from them. if they are equal to some other line we remove it. Therefore we can be quite sure that we don't have very similiar drug ads where only the amount of the drug or something like this changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Before Start/End Duplicate Removal\n",
      "(32893, 6)\n",
      "Shape After Start/End Duplicate Removal\n",
      "(24460, 6)\n"
     ]
    }
   ],
   "source": [
    "grams['descr_start'] = grams['description'].str[:100].apply(lambda text: re.sub(r'[^A-Za-z]+', '', text))\n",
    "print('Shape Before Start/End Duplicate Removal')\n",
    "print(str(grams.shape))\n",
    "grams.drop_duplicates('descr_start', inplace=True)\n",
    "print('Shape After Start/End Duplicate Removal')\n",
    "print(str(grams.shape))\n",
    "\n",
    "#Remove helper Columns\n",
    "del grams['descr_start'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudonymize \n",
    "Remove telephone numbers and vendor names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a random phone number\n",
    "def rand_phone():\n",
    "    return \"+%d %d %d %d\" % (random.randint(20,99), random.randint(100,999), random.randint(1000,9999), random.randint(100,999))\n",
    "\n",
    "\n",
    "# create random name\n",
    "def get_fake_name():\n",
    "    # can create fake names\n",
    "    fake = Faker()\n",
    "    fake_name = fake.name().replace(\" \", \"\")\n",
    "    if random.randint(1, 10) % 2 == 0:\n",
    "        fake_name = fake_name.lower()\n",
    "    if random.randint(1, 10) % 3 == 0:\n",
    "        fake_name = \"%s%d\" % (fake_name.lower(), random.randint(10, 99))\n",
    "    return fake_name\n",
    "\n",
    "\n",
    "# \n",
    "def pseudonymize(df, name_column): \n",
    "    # create a list of all vendor names with different Capitalisation\n",
    "    names_list = df['vendor'].unique()\n",
    "    names_list = [re.escape(str(x)) for x in names_list] # OLD [str(x) for x in names_list] \n",
    "    capitalize = [x.capitalize() for x in names_list]\n",
    "    lower = [x.lower() for x in names_list]\n",
    "    upper = [x.upper() for x in names_list]\n",
    "    names_list += capitalize + lower + upper\n",
    "    \n",
    "    \n",
    "    if 'vendor' in df.columns:\n",
    "        # remove empty strings\n",
    "        df.replace('', np.nan, inplace=True)\n",
    "        \n",
    "        # replace all vendor names by dummie\n",
    "        name_replacements = {name: get_fake_name() for name in names_list  if name is not np.nan}\n",
    "        df.replace({name_column: name_replacements}, inplace=True)\n",
    "        df.replace({\"description\": name_replacements}, inplace=True, regex=True)\n",
    "       \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all phonenumbers \n",
    "all_phonenumbers = [match.raw_string for match in phonenumbers.PhoneNumberMatcher(\" \".join(grams.description.unique()), \"INTERNATIONAL\")]\n",
    "all_phonenumbers = (list(set(all_phonenumbers)))\n",
    "# Replace all phonenumbers\n",
    "for number in all_phonenumbers:\n",
    "    grams.description = grams.description.apply(lambda x:x.replace(number,rand_phone()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_mails(text):\n",
    "    domain = ['.com','.de','.ru','.org']\n",
    "    vendor = ['gmail', 'air','wing','microsoft','hotmail','outlook']\n",
    "\n",
    "    mail = get_fake_name() + '@' + vendor[random.randint(0,len(vendor)-1)] + domain[random.randint(0,len(domain)-1)] \n",
    "    return re.sub(r'\\S+@\\S+\\s?', mail, text)\n",
    "\n",
    "grams.description = grams.description.apply(lambda text: replace_mails(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudonymisation is currently not needed, since we don't publish the DAPT data right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 44s, sys: 1min 2s, total: 13min 46s\n",
      "Wall time: 13min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# vendor name  bcdirect is replaced by random name \n",
    "#\n",
    "grams_pseudo = pseudonymize(grams.copy(),\"vendor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanza Processing \n",
    "\n",
    "### Create special char remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted_elements(text):\n",
    "    final_text=text\n",
    "    final_text = re.sub(r'https?:\\/\\/\\S*[\\r\\n]*', '', final_text)\n",
    "    final_text = re.sub(r'\\S*.onion\\S*[\\r\\n]*', '', final_text)\n",
    "    final_text = re.sub(r'[\\+!~@#$%^&*()={}\\[\\]:;<.>?\\'\"]', '', final_text)\n",
    "    final_text = re.sub(r'[-]+', '-', final_text)\n",
    "    final_text = re.sub(r'[_]+', '_', final_text)\n",
    "    return final_text\n",
    "\n",
    "\n",
    "\n",
    "@register_processor(\"customcleaner\")\n",
    "class cleanerProcessor(Processor):\n",
    "    ''' Processor removes all special chars we do not appreciate and links '''\n",
    "    _requires = set(['tokenize']) # Shouldn#t we do that before\n",
    "    _provides = set(['specialCharRemover'])\n",
    "\n",
    "    def __init__(self, config, pipeline, use_gpu):\n",
    "        pass\n",
    "\n",
    "    def _set_up_model(self, *args):\n",
    "        pass\n",
    "\n",
    "    def process(self, doc):\n",
    "        \n",
    "        doc.text = doc.text # Question to Gabor SHALL the original text stay the same?\n",
    "        for sent in doc.sentences:\n",
    "            for tok in sent.tokens:\n",
    "                tok.text = remove_unwanted_elements(tok.text)\n",
    "                \n",
    "            for word in sent.words:\n",
    "                word.text = remove_unwanted_elements(word.text)\n",
    "\n",
    "                \n",
    "            #remove empty tokens/words    \n",
    "            sent.tokens = [tok for tok in sent.tokens if len(tok.text) > 0]\n",
    "            sent.words = [word for word in sent.words if len(word.text) > 0]\n",
    "        #remove empty sentences\n",
    "        doc.sentences = [sent for sent in doc.sentences if len(sent.tokens) > 0]\n",
    "\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanza Segmentation\n",
    "\n",
    "TODO Talk with Gabor if we should really separate sentences according to Stanza.. Imho this will result in a lot of small sentences with very little context. Does this matter? In case of  Annotating something like Drug_mention it would.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-26 09:22:51 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor     | Package |\n",
      "---------------------------\n",
      "| tokenize      | ewt     |\n",
      "| customcleaner | default |\n",
      "===========================\n",
      "\n",
      "2021-02-26 09:22:51 INFO: Use device: gpu\n",
      "2021-02-26 09:22:51 INFO: Loading: tokenize\n",
      "2021-02-26 09:22:57 INFO: Loading: customcleaner\n",
      "2021-02-26 09:22:57 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = stanza.Pipeline('en', processors='tokenize,customcleaner')#,specialchars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 54s, sys: 4.61 s, total: 15min 58s\n",
      "Wall time: 15min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grams_pseudo['doc'] =  grams_pseudo.description.apply(lambda text:  tokenizer(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore Stanza to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_tokenized_text(doc):\n",
    "    cleaned_text= \"\"\n",
    "    for sentence in doc.sentences:\n",
    "        for token_dict in sentence.words:\n",
    "            cleaned_text += token_dict.text + \" \"\n",
    "        cleaned_text = cleaned_text.strip() + \". \"\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "grams_pseudo[\"clean_description\"] = grams_pseudo[\"doc\"].apply(lambda doc:  restore_tokenized_text(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24460, 7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle the dataset to prevent any order with meaning\n",
    "df_shuffled=grams_pseudo.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "df_shuffled.to_pickle('intermediate_store/preprocessed_DAPT.pickle')\n",
    "df_shuffled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled = pd.read_pickle('intermediate_store/preprocessed_DAPT.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write clean descriptions to file \n",
    "TODO --> Model architecture could be improved to [CLS]name[SEP]Clean_description, since we probably have to cut down bigger parts for the language model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled['description_combi'] = df_shuffled['name'] + '[SEP]' + df_shuffled['clean_description']\n",
    "    \n",
    "#df_shuffled['dream_id', 'name', 'clean_description', 'description_combi']].to_csv(\"final_data/DAPT1.0.tsv\", sep = '\\t')\n",
    "df_shuffled[\"clean_description\"].to_csv(\"final_data/DAPT_Grams_1.0.txt\", sep=' ', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NER36",
   "language": "python",
   "name": "ner36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
