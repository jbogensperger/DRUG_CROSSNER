{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate how much was changed in the Review process of the Crowdsourced Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Reviewed Dataset\n",
    "with open('reviewed_data/initial_1300.json') as json_file:\n",
    "    rewiewed_json = json.load(json_file)\n",
    "    \n",
    "review_dict = {}\n",
    "for row in rewiewed_json:\n",
    "    review_dict[row['data']['dream_id']] = {'completions':row['completions'], 'data':row['data'] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load original exports\n",
    "labelstudio_export_list = ['appen_results/label_studio_Aggregated_Report_1726283.json',\n",
    "'appen_results/label_studio_Aggregated_Report_1726283_2.json',\n",
    "'appen_results/label_studio_Aggregated_Report_1726283_3.json',\n",
    "'appen_results/label_studio_Aggregated_Report_1734122.json'\n",
    "                      ]\n",
    "export_dict = {}\n",
    "\n",
    "for file_path in labelstudio_export_list:\n",
    "    \n",
    "    with open(file_path) as json_file:\n",
    "        export = json.load(json_file)\n",
    "    \n",
    "    for row in export:\n",
    "        export_dict[row['data']['dream_id']] = {'completions':row['completions'], 'data':row['data'] }\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "880"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "len(export_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880  elements are in the review dataset and  0  elements are not.\n"
     ]
    }
   ],
   "source": [
    "contained = 0\n",
    "not_contained = 0\n",
    "for export_key in export_dict.keys():\n",
    "    if export_key in review_dict:\n",
    "        contained+=1\n",
    "    else:\n",
    "        not_contained +=1\n",
    "print(contained, ' elements are in the review dataset and ', not_contained, ' elements are not.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of  18395  characters included in tags after the review  13647  were already present as is in the export\n",
      "This is  0.7418863821690677 %\n",
      "We are missing  138  rows where I cannot find the original export\n"
     ]
    }
   ],
   "source": [
    "#Evaluate How much of the reviewed annotations was already annotated in the crowdsourcing process                    \n",
    "#Characterwise                        \n",
    "total_overlap = 0\n",
    "total_not_overlap = 0\n",
    "whole_row_missing=0\n",
    "\n",
    "for rev_key in review_dict.keys():\n",
    "    if rev_key in export_dict:\n",
    "        export_comp = export_dict[rev_key]['completions']\n",
    "        review_comp = review_dict[rev_key]['completions']\n",
    "        \n",
    "        #Loop over all reviewed annotations\n",
    "        for rev_tag in review_comp[0]['result']:\n",
    "            r_span = range(rev_tag['value']['start'], rev_tag['value']['end'] - 1)\n",
    "            r_spanset = set(r_span)\n",
    "            tag_overlap = 0\n",
    "            tag_not_overlap=0\n",
    "            \n",
    "            if rev_tag['value']['labels'][0] == 'Drug':#only check drug annotations\n",
    "                #check each annotation in original export if it overlaps..\n",
    "                for exp_tag in export_comp[0]['result']:\n",
    "                    exp_span = range(exp_tag['value']['start'], exp_tag['value']['end'] - 1)\n",
    "                    tag_overlap += len(r_spanset.intersection(exp_span))\n",
    "                \n",
    "                #Calculate what was not found in the original annotation and store mikro results\n",
    "                tag_not_overlap = len(r_span) - tag_overlap\n",
    "                total_overlap += tag_overlap\n",
    "                total_not_overlap += tag_not_overlap\n",
    "                if tag_not_overlap < 0:\n",
    "                    print('ERROR')\n",
    "                    \n",
    "    else:\n",
    "        whole_row_missing+=1\n",
    "\n",
    "total_chars = total_overlap + total_not_overlap\n",
    "print('Of ', total_chars, ' characters included in tags after the review ', total_overlap, \n",
    "      ' were already present as is in the export')\n",
    "print('This is ', total_overlap/total_chars, '%')\n",
    "print('We are missing ', whole_row_missing, ' rows where I cannot find the original export')\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of  13769  characters orginally annotated  13647  still annotated after the review\n",
      "This is  0.9911395163047425 %\n",
      "We are missing  0  rows where I cannot find the original export\n"
     ]
    }
   ],
   "source": [
    "#Evaluate How much of the original crowdsourcing annotations were found in the reviewed dataset.   \n",
    "#Characterwise\n",
    "\n",
    "total_overlap = 0\n",
    "total_not_overlap = 0\n",
    "whole_row_missing=0\n",
    "\n",
    "for export_key in export_dict.keys():\n",
    "    if export_key in review_dict:\n",
    "        export_comp = export_dict[export_key]['completions']\n",
    "        review_comp = review_dict[export_key]['completions']\n",
    "        \n",
    "        #Loop over all reviewed annotations\n",
    "        for exp_tag in export_comp[0]['result']:\n",
    "            e_span = range(exp_tag['value']['start'], exp_tag['value']['end'] - 1)\n",
    "            e_spanset = set(e_span)\n",
    "            tag_overlap = 0\n",
    "            tag_not_overlap=0\n",
    "            \n",
    "            if exp_tag['value']['labels'][0] == 'Drug':#only check drug annotations\n",
    "                #check each annotation in original export if it overlaps..\n",
    "                for rev_tag in review_comp[0]['result']:\n",
    "                    r_span = range(rev_tag['value']['start'], rev_tag['value']['end'] - 1)\n",
    "                    tag_overlap += len(e_spanset.intersection(r_span))\n",
    "                \n",
    "                #Calculate what was not found in the original annotation and store mikro results\n",
    "                tag_not_overlap = len(e_span) - tag_overlap\n",
    "                total_overlap += tag_overlap\n",
    "                total_not_overlap += tag_not_overlap\n",
    "                if tag_not_overlap < 0:\n",
    "                    print('ERROR')\n",
    "                    \n",
    "    else:\n",
    "        whole_row_missing+=1\n",
    "\n",
    "total_chars = total_overlap + total_not_overlap\n",
    "print('Of ', total_chars, ' characters orginally annotated ', total_overlap, \n",
    "      ' still annotated after the review')\n",
    "print('This is ', total_overlap/total_chars, '%')\n",
    "print('We are missing ', whole_row_missing, ' rows where I cannot find the original export')\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From a total of  2000  tags  1791  remained unchanged after the review\n",
      "This is  0.8955 %\n"
     ]
    }
   ],
   "source": [
    "#Evaluate How much of the original crowdsourcing annotations were found in the reviewed dataset.   \n",
    "#This reports based on the span hash ids --> only 100% unchanged spans are counted as same\n",
    "amt_exp = 0\n",
    "amt_not_exp = 0\n",
    "\n",
    "for export_key in export_dict.keys():\n",
    "    if export_key in review_dict:\n",
    "        export_comp = export_dict[export_key]['completions']\n",
    "        review_comp = review_dict[export_key]['completions']\n",
    "        \n",
    "        for exp_tag in export_comp[0]['result']:\n",
    "            contained=False\n",
    "            if exp_tag['value']['labels'][0] == 'Drug':\n",
    "            \n",
    "                for rev_tag in review_comp[0]['result']:\n",
    "                    if exp_tag['id'] == rev_tag['id']:\n",
    "                        contained = True\n",
    "\n",
    "                if contained:\n",
    "                    amt_exp += 1\n",
    "                else:\n",
    "                    amt_not_exp += 1\n",
    "\n",
    "total_exp_tags = amt_exp + amt_not_exp\n",
    "print('From a total of ', total_exp_tags, ' tags ', amt_exp, ' remained unchanged after the review')    \n",
    "print('This is ', amt_exp/total_exp_tags, '%')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of  2470  review tags  1791  were already present as is in the export\n",
      "This is  0.7251012145748987 %\n",
      "We are missing  138  rows where I cannot find the original export\n"
     ]
    }
   ],
   "source": [
    "#Evaluate How much of the reviewed annotations was already annotated in the crowdsourcing process        \n",
    "#This reports based on the span hash ids --> only 100% unchanged spans are counted as same\n",
    "amt_rev = 0\n",
    "amt_not_rev = 0\n",
    "whole_row_missing=0\n",
    "\n",
    "for rev_key in review_dict.keys():\n",
    "    if rev_key in export_dict:\n",
    "        export_comp = export_dict[rev_key]['completions']\n",
    "        review_comp = review_dict[rev_key]['completions']\n",
    "        \n",
    "        for rev_tag in review_comp[0]['result']:\n",
    "            contained=False\n",
    "            if rev_tag['value']['labels'][0] == 'Drug':\n",
    "                for exp_tag in export_comp[0]['result']:\n",
    "                    if exp_tag['id'] == rev_tag['id']:\n",
    "                        contained = True\n",
    "\n",
    "                if contained:\n",
    "                    amt_rev += 1\n",
    "                else:\n",
    "                    amt_not_rev += 1\n",
    "    else:\n",
    "        whole_row_missing+=1\n",
    "\n",
    "total_rev_tags = amt_rev + amt_not_rev\n",
    "print('Of ', total_rev_tags, ' review tags ', amt_rev, ' were already present as is in the export')\n",
    "print('This is ', amt_rev/total_rev_tags, '%')\n",
    "print('We are missing ', whole_row_missing, ' rows where I cannot find the original export')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_row_missing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NER36",
   "language": "python",
   "name": "ner36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
