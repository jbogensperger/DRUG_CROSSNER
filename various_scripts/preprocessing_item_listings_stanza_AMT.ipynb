{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing of Drug Dataset From Dreammarket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import langdetect as ld\n",
    "import stanza\n",
    "from stanza.pipeline.processor import register_processor, Processor\n",
    "\n",
    "import time\n",
    "from faker import Faker\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import phonenumbers\n",
    "import random\n",
    "import csv\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset and remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:  Index(['dream_id', 'vendor', 'name', 'description'], dtype='object')  shape:  (45446, 4)\n"
     ]
    }
   ],
   "source": [
    "path_dream_json = \"Drug_export_50000_final.json\"\n",
    "dnm = pd.read_json(path_dream_json)\n",
    "\n",
    "dnm.rename(columns = {'idproduct':'dream_id', 'seller_name':'vendor', 'product_name':'name', 'category':'subcategory' }, inplace=True)\n",
    "\n",
    "DrugColsToKeep = [ 'dream_id', 'vendor', 'name', 'description',]\n",
    "dnm = dnm[DrugColsToKeep]\n",
    "print(\"Columns: \", dnm.columns, \" shape: \", dnm.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines before duplicate removal: 45446\n",
      "lines After duplicate removal: 20434\n"
     ]
    }
   ],
   "source": [
    "#Duplicate removal:\n",
    "print(\"lines before duplicate removal:\", dnm.shape[0])\n",
    "dnm.drop_duplicates('description', inplace=True)\n",
    "print(\"lines After duplicate removal:\", dnm.shape[0])\n",
    "\n",
    "#Create unique index again\n",
    "dnm.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove other languages, empty and non-string descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 22s, sys: 16.7 s, total: 4min 38s\n",
      "Wall time: 4min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def checkLang(text):\n",
    "    try:\n",
    "        if isinstance(text, str):\n",
    "            return ld.detect(text)\n",
    "        else:\n",
    "            return 'No String'\n",
    "    except:  \n",
    "        return 'Error in detect'\n",
    "\n",
    "dnm['lang'] = dnm.apply(lambda row: checkLang(row['description']), axis=1) # TODO shall we work with other languages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Before Language Removal\n",
      "(20434, 5)\n",
      "Shape AFTER Language Removal\n",
      "(18326, 5)\n"
     ]
    }
   ],
   "source": [
    "print('Shape Before Language Removal')\n",
    "print(str(dnm.shape))\n",
    "dnm_en = dnm[dnm['lang'] == 'en'].copy()\n",
    "print('Shape AFTER Language Removal')\n",
    "print(str(dnm_en.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove very short/long texts or texts with similiar start\n",
    "\n",
    "Calculating the edit distance is super cumbersume --> approx. a day runtime with only edit distance (and edit distance doesn't) Take into account that a char can be inserted.. (\"apple\" and \"1apple\" has an edit distance of 5)\n",
    "levenstein or hamming etc. (which compensate for inserted chars) would take even longer.. \n",
    "Therefore, this small heuristc was found to remove texts where the beginning is equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Before Short Descr. Removal\n",
      "(18326, 5)\n",
      "Shape After Short Descr. Removal\n",
      "(16844, 5)\n"
     ]
    }
   ],
   "source": [
    "#Very small descriptions are not interesting for us since we try to find named entities in sentences..\n",
    "print('Shape Before Short Descr. Removal')\n",
    "print(str(dnm_en.shape))\n",
    "dnm_en = dnm_en[dnm_en['description'].map(len) > 30].copy()\n",
    "dnm_en = dnm_en[dnm_en['description'].map(len) < 3000].copy()#Suuper long texts usually contain noisy crap\n",
    "print('Shape After Short Descr. Removal')\n",
    "print(str(dnm_en.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We take the first 200 chars and remove everything non-alphabetical from them. if they are equal to some other line we remove it. Therefore we can be quite sure that we don't have very similiar drug ads where only the amount of the drug or something like this changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Before Start/End Duplicate Removal\n",
      "(16844, 6)\n",
      "Shape After Start/End Duplicate Removal\n",
      "(11674, 6)\n"
     ]
    }
   ],
   "source": [
    "dnm_en['descr_start'] = dnm_en['description'].str[:100].apply(lambda text: re.sub(r'[^A-Za-z]+', '', text))\n",
    "print('Shape Before Start/End Duplicate Removal')\n",
    "print(str(dnm_en.shape))\n",
    "dnm_en.drop_duplicates('descr_start', inplace=True)\n",
    "print('Shape After Start/End Duplicate Removal')\n",
    "print(str(dnm_en.shape))\n",
    "\n",
    "#Remove helper Columns\n",
    "del dnm_en['descr_start'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudonymize \n",
    "Remove telephone numbers and vendor names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a random phone number\n",
    "def rand_phone():\n",
    "    return \"+%d %d %d %d\" % (random.randint(20,99), random.randint(100,999), random.randint(1000,9999), random.randint(100,999))\n",
    "\n",
    "\n",
    "# create random name\n",
    "def get_fake_name():\n",
    "    # can create fake names\n",
    "    fake = Faker()\n",
    "    fake_name = fake.name().replace(\" \", \"\")\n",
    "    if random.randint(1, 10) % 2 == 0:\n",
    "        fake_name = fake_name.lower()\n",
    "    if random.randint(1, 10) % 3 == 0:\n",
    "        fake_name = \"%s%d\" % (fake_name.lower(), random.randint(10, 99))\n",
    "    return fake_name\n",
    "\n",
    "\n",
    "# \n",
    "def pseudonymize(df, name_column): \n",
    "    # create a list of all vendor names with different Capitalisation\n",
    "    names_list = df['vendor'].unique()\n",
    "    names_list = [re.escape(str(x)) for x in names_list] # OLD [str(x) for x in names_list] \n",
    "    capitalize = [x.capitalize() for x in names_list]\n",
    "    lower = [x.lower() for x in names_list]\n",
    "    upper = [x.upper() for x in names_list]\n",
    "    names_list += capitalize + lower + upper\n",
    "    \n",
    "    \n",
    "    if 'vendor' in df.columns:\n",
    "        # remove empty strings\n",
    "        df.replace('', np.nan, inplace=True)\n",
    "        \n",
    "        # replace all vendor names by dummie\n",
    "        name_replacements = {name: get_fake_name() for name in names_list  if name is not np.nan}\n",
    "        df.replace({name_column: name_replacements}, inplace=True)\n",
    "        df.replace({\"description\": name_replacements}, inplace=True, regex=True)\n",
    "       \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all phonenumbers \n",
    "all_phonenumbers = [match.raw_string for match in phonenumbers.PhoneNumberMatcher(\" \".join(dnm_en.description.unique()), \"INTERNATIONAL\")]\n",
    "all_phonenumbers = (list(set(all_phonenumbers)))\n",
    "# Replace all phonenumbers\n",
    "for number in all_phonenumbers:\n",
    "    dnm_en.description = dnm_en.description.apply(lambda x:x.replace(number,rand_phone()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_mails(text):\n",
    "    domain = ['.com','.de','.ru','.org']\n",
    "    vendor = ['gmail', 'air','wing','microsoft','hotmail','outlook']\n",
    "\n",
    "    mail = get_fake_name() + '@' + vendor[random.randint(0,len(vendor)-1)] + domain[random.randint(0,len(domain)-1)] \n",
    "    return re.sub(r'\\S+@\\S+\\s?', mail, text)\n",
    "\n",
    "dnm_en.description = dnm_en.description.apply(lambda text: replace_mails(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 14s, sys: 35.8 s, total: 6min 50s\n",
      "Wall time: 6min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# vendor name  bcdirect is replaced by random name \n",
    "\n",
    "dnm_pseudo = pseudonymize(dnm_en.copy(),\"vendor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanza Processing \n",
    "\n",
    "### Create special char remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted_elements(text):\n",
    "    final_text=text\n",
    "    final_text = re.sub(r'https?:\\/\\/\\S*[\\r\\n]*', '', final_text)\n",
    "    final_text = re.sub(r'\\S*.onion\\S*[\\r\\n]*', '', final_text)\n",
    "    final_text = re.sub(r'[\\+!~@#$%^&*()={}\\[\\]:;<.>?\\'\"]', '', final_text)\n",
    "    \n",
    "    #Due to AMT restrictions.. replace \\' \\\" and ',' with their unicode descriptor\n",
    "    final_text = re.sub(',', '&#44', final_text)\n",
    "    final_text = re.sub('\\'', '&#39', final_text)\n",
    "    final_text = re.sub('\\\"', '&#34', final_text)\n",
    "    \n",
    "    final_text = re.sub(r'[-]+', '-', final_text)\n",
    "    final_text = re.sub(r'[_]+', '_', final_text)\n",
    "    return final_text\n",
    "\n",
    "\n",
    "\n",
    "@register_processor(\"customcleaner\")\n",
    "class cleanerProcessor(Processor):\n",
    "    ''' Processor removes all special chars we do not appreciate and links '''\n",
    "    _requires = set(['tokenize']) # Shouldn#t we do that before\n",
    "    _provides = set(['specialCharRemover'])\n",
    "\n",
    "    def __init__(self, config, pipeline, use_gpu):\n",
    "        pass\n",
    "\n",
    "    def _set_up_model(self, *args):\n",
    "        pass\n",
    "\n",
    "    def process(self, doc):\n",
    "        \n",
    "        doc.text = doc.text # Question to Gabor SHALL the original text stay the same?\n",
    "        for sent in doc.sentences:\n",
    "            for tok in sent.tokens:\n",
    "                tok.text = remove_unwanted_elements(tok.text)\n",
    "                \n",
    "            for word in sent.words:\n",
    "                word.text = remove_unwanted_elements(word.text)\n",
    "\n",
    "                \n",
    "            #remove empty tokens/words    \n",
    "            sent.tokens = [tok for tok in sent.tokens if len(tok.text) > 0]\n",
    "            sent.words = [word for word in sent.words if len(word.text) > 0]\n",
    "        #remove empty sentences\n",
    "        doc.sentences = [sent for sent in doc.sentences if len(sent.tokens) > 0]\n",
    "\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanza Segmentation\n",
    "\n",
    "TODO Talk with Gabor if we should really separate sentences according to Stanza.. Imho this will result in a lot of small sentences with very little context. Does this matter? In case of  Annotating something like Drug_mention it would.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-04 10:27:10 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor     | Package |\n",
      "---------------------------\n",
      "| tokenize      | ewt     |\n",
      "| customcleaner | default |\n",
      "===========================\n",
      "\n",
      "2021-03-04 10:27:10 INFO: Use device: gpu\n",
      "2021-03-04 10:27:10 INFO: Loading: tokenize\n",
      "2021-03-04 10:27:16 INFO: Loading: customcleaner\n",
      "2021-03-04 10:27:16 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = stanza.Pipeline('en', processors='tokenize,customcleaner')#,specialchars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 43s, sys: 1.91 s, total: 7min 45s\n",
      "Wall time: 7min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dnm_pseudo['doc'] =  dnm_pseudo.description.apply(lambda text:  tokenizer(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore Stanza to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_tokenized_text(doc):\n",
    "    cleaned_text= \"\"\n",
    "    for sentence in doc.sentences:\n",
    "        for token_dict in sentence.words:\n",
    "            cleaned_text += token_dict.text + \" \"\n",
    "        cleaned_text = cleaned_text.strip() + \". \"\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "dnm_pseudo[\"clean_description\"] = dnm_pseudo[\"doc\"].apply(lambda doc:  restore_tokenized_text(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11674, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle the dataset to prevent any order with meaning\n",
    "df_shuffled=dnm_pseudo.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "#df_shuffled.to_pickle('intermediate_store/preprocessed_AMT_ads_V1.2.pickle')\n",
    "dnm_pseudo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stanza'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6573f82b039b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_processor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stanza'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import stanza\n",
    "from stanza.pipeline.processor import register_processor, Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stanza'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/storage/envs/general/lib/python3.9/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mexcs_to_catch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stanza'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-dc07d1cda6fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_shuffled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'intermediate_store/preprocessed_AMT_ads_V1.2.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/storage/envs/general/lib/python3.9/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;31m#  \"No module named 'pandas.core.sparse.series'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0;31m#  \"Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;31m# e.g. can occur for files written in py27; see GH#28645 and GH#31988\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/storage/envs/general/lib/python3.9/site-packages/pandas/compat/pickle_compat.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fh, encoding, is_verbose)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_verbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/storage/envs/general/lib/python3.9/pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1210\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/storage/envs/general/lib/python3.9/pickle.py\u001b[0m in \u001b[0;36mload_stack_global\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"STACK_GLOBAL requires str\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1538\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSTACK_GLOBAL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_stack_global\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/storage/envs/general/lib/python3.9/site-packages/pandas/compat/pickle_compat.py\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_class_locations_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/storage/envs/general/lib/python3.9/pickle.py\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m   1577\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_compat_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMPORT_MAPPING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compat_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMPORT_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1579\u001b[0;31m         \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_getattribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stanza'"
     ]
    }
   ],
   "source": [
    "df_shuffled = pd.read_pickle('intermediate_store/preprocessed_AMT_ads_V1.2.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove ITEM already annotated by APPEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load reviewed annotation data\n",
    "with open('reviewed_data/initial_1300.json') as json_file:\n",
    "    rewiewed_json = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_dream_ids = []\n",
    "\n",
    "for review in rewiewed_json:\n",
    "    annotated_dream_ids.append(review['data']['dream_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11674, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shuffled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10661, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shuffled = df_shuffled[~df_shuffled['dream_id'].isin(annotated_dream_ids)]\n",
    "df_shuffled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        ONLY DOMESTIC BIG BUD XXL Very GOOD Quality Ca...\n",
       "2        Durgamata is a precious strain &#44 it s has a...\n",
       "3        Goodfellers is back. 4500 sales on AB always p...\n",
       "4        This listing is for. Pregabalin 300 mg x 56 Ta...\n",
       "5        Direct from US pharmacy. Real Adderall - not P...\n",
       "                               ...                        \n",
       "11669    This is a custom listing for previously approv...\n",
       "11670    New batch of ketamine shards this time. Pure c...\n",
       "11671    14g TOTAL. 7g HOMEGROWN BIG BUDDHA BLUE CHEESE...\n",
       "11672    This is for 20 OXYCONTIN 40 mg pills just like...\n",
       "11673    VitaminClub proudly offers you the highest and...\n",
       "Name: clean_description, Length: 10661, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shuffled.clean_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate item listings according to length into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_textLength(text):\n",
    "    if len(text)<100:\n",
    "        return 1\n",
    "    elif len(text)<250:\n",
    "        return 2\n",
    "    elif len(text)<500:\n",
    "        return 3\n",
    "    elif len(text)<750:\n",
    "        return 4\n",
    "    elif len(text)<1000:\n",
    "        return 5\n",
    "    elif len(text)<1500:\n",
    "        return 6\n",
    "    elif len(text)<2000:\n",
    "        return 7\n",
    "    elif len(text)<2500:\n",
    "        return 8\n",
    "    elif len(text)<3000:\n",
    "        return 9\n",
    "    \n",
    "df_shuffled[\"textLengthCat\"] = df_shuffled[\"clean_description\"].apply(lambda text: categorize_textLength(text))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled['clean_name']= df_shuffled['name']\n",
    "\n",
    "df_shuffled['clean_name'] = df_shuffled['clean_name'].str.replace(',', '&#44')\n",
    "df_shuffled['clean_name'] = df_shuffled['clean_name'].str.replace('\\'', '&#39')\n",
    "df_shuffled['clean_name'] = df_shuffled['clean_name'].str.replace('\"', '&#34')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the combination for the name\n",
    "df_shuffled[\"dream_id_name\"] = df_shuffled[\"dream_id\"].astype(str) +\" - \"+ df_shuffled[\"clean_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_length(doc):\n",
    "    length=0\n",
    "    doc_dict = doc.to_dict()\n",
    "    for sent in doc_dict:\n",
    "        length+=len(sent)\n",
    "    return length\n",
    "#testdoc = df_shuffled.iloc[1]\n",
    "#get_doc_length(testdoc.doc)\n",
    "pd.set_option('display.max_rows', df_shuffled.shape[0])\n",
    "#df_shuffled.doc.apply(lambda doc: get_doc_length(doc)).sort_values(ascending=False)\n",
    "df_shuffled['doc_len'] = df_shuffled.doc.apply(lambda doc: get_doc_length(doc))\n",
    "#TODO remove super long ones in the future.\n",
    "#Hui Hui Hui das war knapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 11)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shuffled=df_shuffled[df_shuffled['textLengthCat'] <= 5].copy()\n",
    "df_shuffled=df_shuffled[1600:1900].copy()\n",
    "#:100 in first batch\n",
    "#200:500 in second batch\n",
    "#500:1100 in thrid batch\n",
    "#1100:1600 in fourth batch\n",
    "#1600:1900 in fifth batch\n",
    "\n",
    "\n",
    "#df_shuffled=df_shuffled[(df_shuffled['textLengthCat'] > 5 ) & (df_shuffled['doc_len'] <= 509)].copy()\n",
    "#df_shuffled=df_shuffled[500:700].copy()\n",
    "#Long Batches\n",
    "#:100 in first batch\n",
    "#100:300 in second batch\n",
    "#300:500 in third batch\n",
    "#500:700 in fourth batch\n",
    "\n",
    "\n",
    "df_shuffled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled[df_shuffled['textLengthCat'] <= 5][['dream_id_name', 'clean_description']].to_csv(\"final_data/Batch_1600_1900_AMT.csv\", \n",
    "                                                                                                 sep = ',', quotechar='\\'', index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "#df_shuffled[['dream_id_name',  'clean_description']].to_csv(\"final_data/LongBatch_500_700_AMT.csv\", \n",
    "#                                                                                                sep = ',', quotechar='\\'', index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 11)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shuffled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "general"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
