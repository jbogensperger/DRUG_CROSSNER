{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import time\n",
    "from itertools import combinations\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Full Report Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Pickle File: Full_Report1722845.csv\n",
      "Loaded Pickle File: Full_Report_1726283.csv\n",
      "Loaded Pickle File: Full_Report_1726283_2.csv\n",
      "Loaded Pickle File: Full_Report_1726283_3.csv\n",
      "Loaded Pickle File: Full_Report_1734122.csv\n"
     ]
    }
   ],
   "source": [
    "run1 = \"Full_Report1722845.csv\"\n",
    "run3 = \"Full_Report_1726283.csv\"\n",
    "run4 = \"Full_Report_1726283_2.csv\"\n",
    "run5 = \"Full_Report_1726283_3.csv\"\n",
    "run6 = \"Full_Report_1734122.csv\"\n",
    "\n",
    "\n",
    "\n",
    "report_name=run6\n",
    "\n",
    "\n",
    "#full_report = pd.read_csv(\"appen_results/\" + report_name)\n",
    "\n",
    "all_files = [\"Full_Report1722845.csv\", \n",
    "     \"Full_Report_1726283.csv\", \n",
    "     \"Full_Report_1726283_2.csv\", \n",
    "     \"Full_Report_1726283_3.csv\", \n",
    "     \"Full_Report_1734122.csv\"]\n",
    "\n",
    "full_report=None\n",
    "for report_name in all_files:\n",
    "    if not os.path.exists('intermediate_store/' + report_name + '.pickle'):\n",
    "        raise Exception(\"File Missing:\", report_name)\n",
    "    else:\n",
    "        if full_report is not None:\n",
    "            read = pd.read_pickle('intermediate_store/' + report_name + '.pickle')\n",
    "            full_report = pd.concat([full_report, read], axis=0)\n",
    "        else:\n",
    "            full_report = pd.read_pickle('intermediate_store/' + report_name + '.pickle')\n",
    "        print(\"Loaded Pickle File:\", report_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download JSONs, since Appen only delivers a link.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Pickle File\n",
      "CPU times: user 1.5 s, sys: 292 ms, total: 1.8 s\n",
      "Wall time: 1.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#The annotations are for some reasons only embedded as link --> extra download needed.\n",
    "def download_json(link, pandas_id):\n",
    "    print(pandas_id)\n",
    "    \n",
    "    try:\n",
    "        data = urllib.request.urlopen(link).read()\n",
    "        output = json.loads(data)\n",
    "        return output\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "    \n",
    "\n",
    "\n",
    "#And Persist Results, since we don't wanna do this every time...\n",
    "if not os.path.exists('intermediate_store/' + report_name + '.pickle'):\n",
    "    full_report[\"anno_json\"] = full_report.apply(lambda row: download_json(row['annotations'], row.name ), axis=1)\n",
    "    full_report.to_pickle('intermediate_store/' + report_name + '.pickle')\n",
    "    print(\"Created new Pickle File\")\n",
    "else:\n",
    "    full_report = pd.read_pickle('intermediate_store/' + report_name + '.pickle')\n",
    "    full_report.anno_json.head(1)\n",
    "    print(\"Loaded Pickle File\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decide whether only new annotation or also agreement between gold annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 21)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_annotations = full_report[full_report['_golden'] == False].copy()\n",
    "#new_annotations = full_report.copy()\n",
    "new_annotations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Amount of Internal channel values : \\n\", new_annotations._channel.value_counts())\n",
    "#new_annotations =  new_annotations[new_annotations['_channel'] != 'cf_internal'].copy()\n",
    "#new_annotations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of tainted values : \n",
      " False    900\n",
      "Name: _tainted, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(900, 21)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Amount of tainted values : \\n\", new_annotations._tainted.value_counts())\n",
    "new_annotations =  new_annotations[new_annotations['_tainted'] == False].copy()\n",
    "new_annotations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove old annotations to check individual results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 21)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_annotations._created_at = pd.to_datetime(new_annotations._created_at)\n",
    "new_annotations = new_annotations[new_annotations['_created_at'] >=  datetime.datetime(2021,2,20,0,0,0)].copy()\n",
    "new_annotations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Performance with F1 Score and Cohen's Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Json to binary annotation list\n",
    "def json_to_list(json):\n",
    "    annotations = []\n",
    "    try:\n",
    "        for i, span in enumerate(json['spans']):\n",
    "            if len(span['classnames']) > 0 and span['classnames'][0] == 'Drug':\n",
    "                annotations.append(1)\n",
    "            elif len(span['classnames']) > 0 and span['classnames'][0] == 'None':\n",
    "                annotations.append(2)\n",
    "            else:\n",
    "                annotations.append(0)\n",
    "    except KeyError:\n",
    "        print(\"error at \", str(json))\n",
    "    return annotations        \n",
    "    \n",
    "new_annotations['anno_list'] = new_annotations[\"anno_json\"].apply(lambda json: json_to_list(json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      None\n",
       "1      None\n",
       "2      None\n",
       "3      None\n",
       "4      None\n",
       "       ... \n",
       "895    None\n",
       "896    None\n",
       "897    None\n",
       "898    None\n",
       "899    None\n",
       "Length: 900, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worker_annotations = {}\n",
    "def write_to_dict(worker_id, unit_id, anno_list):\n",
    "    if worker_id in worker_annotations:\n",
    "        current_elements = worker_annotations[worker_id]\n",
    "        current_elements[unit_id] = anno_list\n",
    "        worker_annotations[worker_id] = current_elements\n",
    "    else:\n",
    "        worker_annotations[worker_id] = {unit_id:anno_list}\n",
    "    \n",
    "    \n",
    "new_annotations.apply(lambda row: write_to_dict(row['_worker_id'], row['_unit_id'], row['anno_list']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credits to Louis de Bruijn (https://towardsdatascience.com/inter-annotator-agreement-2f46c6d37bf3)\n",
    "def cohen_kappa(ann1, ann2):\n",
    "    \"\"\"Computes Cohen kappa for pair-wise annotators.\n",
    "    :param ann1: annotations provided by first annotator\n",
    "    :type ann1: list\n",
    "    :param ann2: annotations provided by second annotator\n",
    "    :type ann2: list\n",
    "    :rtype: float\n",
    "    :return: Cohen kappa statistic\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for an1, an2 in zip(ann1, ann2):\n",
    "        if an1 == an2:\n",
    "            count += 1\n",
    "    A = count / len(ann1)  # observed agreement A (Po)\n",
    "\n",
    "    uniq = set(ann1 + ann2)\n",
    "    E = 0  # expected agreement E (Pe)\n",
    "    for item in uniq:\n",
    "        cnt1 = ann1.count(item)\n",
    "        cnt2 = ann2.count(item)\n",
    "        count = ((cnt1 / len(ann1)) * (cnt2 / len(ann2)))\n",
    "        E += count\n",
    "\n",
    "    return round((A - E) / (1 - E), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FScore formula for inter annotator agreement from: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1090460/\n",
    "\n",
    "micro_fscores={}\n",
    "cohen_kappas = {}\n",
    "unique_worker_combis = list(combinations(new_annotations._worker_id.unique(), 2))\n",
    "\n",
    "#Global Fscore\n",
    "g_a = 0\n",
    "g_b = 0\n",
    "g_c = 0\n",
    "for (worker1, worker2) in unique_worker_combis:\n",
    "    intersection = worker_annotations[worker1].keys() & worker_annotations[worker2].keys()\n",
    "    w1_full_annos = []\n",
    "    w2_full_annos = []\n",
    "    \n",
    "    # Calculate Measures only for overlapping pairs..\n",
    "    if len(intersection) > 0: \n",
    "        #micro Fscores\n",
    "        a = 0\n",
    "        b = 0\n",
    "        c = 0\n",
    "        for item in intersection:\n",
    "            w1_full_annos.extend(worker_annotations[worker1][item])\n",
    "            w2_full_annos.extend(worker_annotations[worker2][item])\n",
    "            for (x, y) in zip( worker_annotations[worker1][item], worker_annotations[worker2][item] ):\n",
    "                if (x == 1 and y == 1) or (x == 2 and y == 2):\n",
    "                    a += 1\n",
    "                    g_a += 1\n",
    "                elif x > y:\n",
    "                    b += 1\n",
    "                    g_b += 1\n",
    "                elif x < y:\n",
    "                    c += 1\n",
    "                    g_c += 1\n",
    "        #print(item, ' a:', a, ' b:', b, 'c:', c)\n",
    "        #F-Score for user pair\n",
    "        if a != 0 or b != 0 or c != 0:\n",
    "            fscore = 2*a /  ( (2*a) + b + c )\n",
    "            micro_fscores[(worker1, worker2)] = fscore\n",
    "            #Cohens Kappa for user pai\n",
    "            cohen_kappas[(worker1, worker2)] = cohen_kappa(w1_full_annos, w2_full_annos)\n",
    "        else:\n",
    "            fscore = 1 #TODO Check if that is really true\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mikro F-Score is:  0.5517805976258698\n",
      "The Makro F-Score is:  0.5993098519827286\n",
      "The Avg. Pairwise Cohen Kappa is:  0.4310162079510703\n",
      " The Average Agreement according to Appen (Test Question Score of users who agreed multiplied / Sum of annotators) 0.897842\n"
     ]
    }
   ],
   "source": [
    "#Report Global F1 Score        \n",
    "global_fscore = 2*g_a /  ( (2*g_a) + g_b + g_c )\n",
    "print('The Mikro F-Score is: ', global_fscore)\n",
    "\n",
    "#Macro Average F1 Score\n",
    "makro_fscore = 0\n",
    "for val in micro_fscores.values(): \n",
    "    makro_fscore += val \n",
    "\n",
    "makro_fscore = makro_fscore / len(micro_fscores)\n",
    "print('The Makro F-Score is: ', makro_fscore)\n",
    "\n",
    "#Macro Averaged Cohens Kappa\n",
    "makro_cohen = 0\n",
    "for val in cohen_kappas.values(): \n",
    "    makro_cohen += val \n",
    "\n",
    "makro_cohen = makro_cohen / len(cohen_kappas) \n",
    "print('The Avg. Pairwise Cohen Kappa is: ', makro_cohen)\n",
    "\n",
    "\n",
    "print(' The Average Agreement according to Appen (Test Question Score of users who agreed multiplied / Sum of annotators)', \n",
    "     new_annotations._trust.mean())\n",
    "\n",
    "#micro_fscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NER36",
   "language": "python",
   "name": "ner36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
